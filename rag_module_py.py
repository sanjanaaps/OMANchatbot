# -*- coding: utf-8 -*-
"""rag_module.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D0F4Eu9Lc7kc6cC2T34npBkqNCiFt71-
"""

!pip install pytesseract
!pip install -U langchain-community
!pip install google-cloud-translate==2.0.0
!pip install pdfplumber
!pip install httpx==0.28.1
!pip install gradio_client==0.14.1 gradio --upgrade
!pip install langchain-huggingface sentence-transformers transformers faiss-cpu pytesseract pdfplumber googletrans websockets deep_translator
# Colab: install libs
!pip install  faiss-cpu PyPDF2 pdfplumber flask flask_cors pyngrok

!pip install --upgrade gradio

import zipfile
import os

# Path to your zip file
zip_path = "/content/Doc_files.zip"

# Directory to extract files
extract_dir = "/content/extracted_files" # Changed directory name

# Create extraction directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Extract zip contents
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"Files extracted to: {extract_dir}")
print("Files:")
print(os.listdir(extract_dir))

# ---------------- Imports ----------------
from langchain_core.documents import Document
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import pytesseract
import pdfplumber
from PIL import Image
from deep_translator import GoogleTranslator
import gradio as gr

# ---------------- Document Processing ----------------
class DocumentProcessor:
    def __init__(self):
        self.ocr_lang = 'eng+ara'  # OCR for English + Arabic
        self.translator_en = GoogleTranslator(source='auto', target='en')
        self.translator_ar = GoogleTranslator(source='auto', target='ar')

    def extract_text_from_pdf(self, filepath):
        text = ""
        with pdfplumber.open(filepath) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text

    def extract_text_from_image(self, filepath):
        return pytesseract.image_to_string(Image.open(filepath), lang=self.ocr_lang)

    def translate_text_in_chunks(self, text, dest='en', chunk_size=4000):
        """Translate large text safely in chunks"""
        translated_text = ""
        translator = self.translator_en if dest=='en' else self.translator_ar
        for i in range(0, len(text), chunk_size):
            chunk = text[i:i+chunk_size]
            translated_text += translator.translate(chunk)
        return translated_text

    def process_document(self, filepath, filetype='pdf'):
        if filetype == 'pdf':
            text = self.extract_text_from_pdf(filepath)
        else:
            text = self.extract_text_from_image(filepath)

        text_en = self.translate_text_in_chunks(text, dest='en')
        text_ar = text  # keep original as Arabic
        summary_en = "Summarization disabled"
        summary_ar = "Summarization disabled"
        return text, text_en, summary_en, summary_ar

# ---------------- Department Tagging ----------------
class DepartmentTagger:
    def __init__(self):
        self.keywords = {
            "Finance": ["budget", "revenue", "expense", "finance"],
            "Currency": ["banknotes", "coins", "mint", "currency"],
            "IT": ["network", "software", "hardware", "technology", "it"],
            "Legal": ["regulation", "law", "compliance", "legal"]
        }

    def tag(self, text):
        txt = text.lower()
        return [dept for dept, kws in self.keywords.items() if any(kw in txt for kw in kws)]

# ---------------- LLM Setup ----------------
def get_llm():
    model_name = "google/flan-t5-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    device = 0 if torch.cuda.is_available() else -1  # GPU if available
    pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=512, device=device)
    print("GPU detected." if device == 0 else "GPU not detected. Using CPU.")
    return HuggingFacePipeline(pipeline=pipe)

# ---------------- Prompt Template ----------------
prompt_template = """You are a helpful assistant for the Oman Central Bank.
Use the context to answer the question accurately and concisely.
If you don't know the answer, say you don't know.

Context: {context}

Question: {question}

Answer:"""

prompt = PromptTemplate(input_variables=["context", "question"], template=prompt_template)

# ---------------- Oman Central Bank RAG ----------------
class OmanCBRAG:
    def __init__(self):
        self.processor = DocumentProcessor()
        self.tagger = DepartmentTagger()
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.vector_store = None
        self.qa_chain = None
        self.llm = get_llm()

    def ingest_documents(self, folder_path):
        documents = []
        # Correcting the nested folder path to the extracted files
        extracted_files_path = os.path.join(folder_path, 'extracted_files', 'cbo currency data set')
        if not os.path.isdir(extracted_files_path):
             raise ValueError(f"Expected folder path but got file path: {extracted_files_path}")
        for file in os.listdir(extracted_files_path):
            if file.lower().endswith(".pdf"):
                fp = os.path.join(extracted_files_path, file)
                text_orig, text_en, sum_en, sum_ar = self.processor.process_document(fp, 'pdf')
                depts = self.tagger.tag(text_en)
                splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
                chunks = splitter.split_text(text_en)
                for chunk in chunks:
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "filename": file,
                            "summary_en": sum_en,
                            "summary_ar": sum_ar,
                            "departments": depts
                        }
                    )
                    documents.append(doc)

        if not documents:
            print(f"No documents processed from folder: {extracted_files_path}")
            return

        self.vector_store = FAISS.from_documents(documents, self.embeddings)
        retriever = self.vector_store.as_retriever(search_type="similarity", search_kwargs={"k":3})
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )
        print(f"Ingested and indexed {len(documents)} chunks from folder {extracted_files_path}")

    def query(self, question, language='en'):
        if not self.qa_chain:
            return "RAG system not initialized. Please ingest documents first.", ""

        result = self.qa_chain({"query": question})
        answer_en = result["result"]

        if language == 'ar':
            answer_ar = self.processor.translate_text_in_chunks(answer_en, dest='ar')
            return answer_ar, answer_en
        else:
            return answer_en, answer_en


# ---------------- Initialize RAG ----------------
rag = OmanCBRAG()
rag.ingest_documents("/content/")  # Update the ingestion path

# ---------------- Gradio Interface ----------------
def gradio_qa_interface(question, language):
    answer, answer_en = rag.query(question, language)
    if language == 'ar':
        return f"الإجابة بالعربية:\n{answer}\n\n(English answer for reference: {answer_en})"
    else:
        return f"English answer:\n{answer}"

with gr.Blocks() as demo:
    gr.Markdown("### Oman Central Bank RAG Chatbot")
    with gr.Row():
        lang_input = gr.Radio(choices=["en", "ar"], label="Language", value="en")
    q_input = gr.Textbox(lines=2, label="Ask your question here")
    answer_box = gr.Textbox(label="Answer", lines=6)
    ask_btn = gr.Button("Get Answer")
    ask_btn.click(fn=gradio_qa_interface, inputs=[q_input, lang_input], outputs=answer_box)

demo.launch(share=True)